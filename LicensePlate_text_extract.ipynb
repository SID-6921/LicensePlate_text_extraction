{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/umairalam567/license?scriptVersionId=144474407\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-27T16:40:48.72493Z","iopub.execute_input":"2023-09-27T16:40:48.725323Z","iopub.status.idle":"2023-09-27T16:40:49.002862Z","shell.execute_reply.started":"2023-09-27T16:40:48.725292Z","shell.execute_reply":"2023-09-27T16:40:49.001808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport os\n\nshutil.copytree('/kaggle/input/car-plate-detection', '/kaggle/working/car-plate-detection')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:40:49.00504Z","iopub.execute_input":"2023-09-27T16:40:49.008023Z","iopub.status.idle":"2023-09-27T16:40:56.258459Z","shell.execute_reply.started":"2023-09-27T16:40:49.007975Z","shell.execute_reply":"2023-09-27T16:40:56.257161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code Explanation\n\nThe code is designed to convert XML annotations (typically used in object detection tasks) to the YOLO (You Only Look Once) format. The YOLO format is a popular format for object detection tasks, especially when using the YOLO architecture.\n\n### Importing Necessary Libraries\n```python\nimport os\nimport xml.etree.ElementTree as ET\nimport shutil\n```\n\n- `os`: This module provides a way of using operating system dependent functionality.\n- `xml.etree.ElementTree`: This module is used for parsing XML data.\n- `shutil`: This module helps in automating the process of copying and removal of files and directories.\n\n### Function: `convert_annotation`\nThis function converts a single XML annotation file to the YOLO format.\n\n- It first parses the XML file to get the root of the XML tree.\n- Extracts the image dimensions (width and height).\n- Iterates over each object in the XML (in this case, looking for objects named 'licence').\n- For each 'licence' object, it extracts the bounding box coordinates.\n- Converts these coordinates to the YOLO format, which is normalized and relative to the image dimensions.\n- Writes the converted annotation to an output file.\n\n### Function: `main`\nThis is the main driver function.\n\n- It first defines the directories for images, XML annotations, and the output directories for training and validation splits.\n- It ensures that the output directories exist, and if not, creates them.\n- It then lists all XML files in the annotation directory.\n- For each XML file, it determines the corresponding image file.\n- Based on a predefined split (80% training, 20% validation), it copies the image to the respective directory (training or validation).\n- Calls the `convert_annotation` function to convert the XML annotation to YOLO format and saves it in the respective directory.\n\nFinally, the `main` function is called to execute the entire process.\n\n### Note:\nThe code assumes that for each XML file, there is a corresponding image with the same name but with a '.png' extension. It also assumes that the only object of interest in the XML files is 'licence', and it assigns this object a class ID of 0 in the YOLO format.","metadata":{}},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport shutil\n\ndef convert_annotation(xml_path, img_path, output_label_path):\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        # Extract image dimensions\n        size = root.find('size')\n        w = int(size.find('width').text)\n        h = int(size.find('height').text)\n\n        # Open output file\n        with open(output_label_path, 'w') as out_file:\n            for obj in root.iter('object'):\n                # Get class name (assuming 'licence' is class 0)\n                cls_name = obj.find('name').text\n                if cls_name == 'licence':\n                    cls_id = 0\n                else:\n                    continue\n\n                # Get bounding box coordinates\n                xmlbox = obj.find('bndbox')\n                b = (float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text),\n                     float(xmlbox.find('xmax').text), float(xmlbox.find('ymax').text))\n                bb = (b[0] + b[2]) / 2 / w, (b[1] + b[3]) / 2 / h, (b[2] - b[0]) / w, (b[3] - b[1]) / h\n\n                # Write to output file in YOLO format\n                out_file.write(str(cls_id) + ' ' + ' '.join([str(a) for a in bb]) + '\\n')\n\n    except Exception as e:\n        print(f'Error processing {xml_path}: {e}')\n\ndef main():\n    # Directories\n    base_dir = '/kaggle/working/car-plate-detection'\n    img_dir = os.path.join(base_dir, 'images')\n    xml_dir = os.path.join(base_dir, 'annotations')\n    \n    train_img_output_dir = os.path.join(base_dir, 'train/images')\n    train_label_output_dir = os.path.join(base_dir, 'train/labels')\n    val_img_output_dir = os.path.join(base_dir, 'val/images')\n    val_label_output_dir = os.path.join(base_dir, 'val/labels')\n\n    # Create output directories if they don't exist\n    for dir_path in [train_img_output_dir, train_label_output_dir, val_img_output_dir, val_label_output_dir]:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n\n    # Process each XML file\n    xml_files = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n    total_files = len(xml_files)\n    train_split = int(0.8 * total_files)  # 80% for training\n\n    for idx, xml_file in enumerate(xml_files):\n        img_file = xml_file.replace('.xml', '.png')\n        if os.path.exists(os.path.join(img_dir, img_file)):\n            if idx < train_split:\n                img_output_path = os.path.join(train_img_output_dir, img_file)\n                label_output_path = os.path.join(train_label_output_dir, img_file.replace('.png', '.txt'))\n            else:\n                img_output_path = os.path.join(val_img_output_dir, img_file)\n                label_output_path = os.path.join(val_label_output_dir, img_file.replace('.png', '.txt'))\n            \n            # Copy image to respective directory\n            shutil.copy(os.path.join(img_dir, img_file), img_output_path)\n            \n            # Convert annotation\n            convert_annotation(os.path.join(xml_dir, xml_file), img_output_path, label_output_path)\n\n\nmain()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:40:56.260216Z","iopub.execute_input":"2023-09-27T16:40:56.260608Z","iopub.status.idle":"2023-09-27T16:40:56.578717Z","shell.execute_reply.started":"2023-09-27T16:40:56.260574Z","shell.execute_reply":"2023-09-27T16:40:56.57765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/car-plate-detection","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:40:56.581906Z","iopub.execute_input":"2023-09-27T16:40:56.582324Z","iopub.status.idle":"2023-09-27T16:40:56.588899Z","shell.execute_reply.started":"2023-09-27T16:40:56.582286Z","shell.execute_reply":"2023-09-27T16:40:56.587802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:40:56.590616Z","iopub.execute_input":"2023-09-27T16:40:56.591271Z","iopub.status.idle":"2023-09-27T16:40:57.577313Z","shell.execute_reply.started":"2023-09-27T16:40:56.591236Z","shell.execute_reply":"2023-09-27T16:40:57.576134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YOLO Model Loading Explanation\n\nThe provided code snippet demonstrates how to load a YOLO model using the `ultralytics` library. The `ultralytics` library provides utilities and functionalities for working with the YOLO (You Only Look Once) object detection architecture.\n\n---\n\n### **1. Importing the YOLO Class**\n\n```python\nfrom ultralytics import YOLO\n```\n\nThis line imports the `YOLO` class from the `ultralytics` library, which is used for creating and managing YOLO models.\n\n---\n\n### **2. Loading a YOLO Model**\n\nThere are multiple ways to load a YOLO model, and the code snippet provides three examples:\n\n1. **Building a New Model from YAML:**\n\n```python\n#model = YOLO('yolov8n.yaml')\n```\n\nThis line (currently commented out) demonstrates how to build a new YOLO model using a YAML configuration file (`yolov8n.yaml`). The YAML file contains the architecture and configuration details of the model.\n\n2. **Loading a Pretrained Model:**\n\n```python\nmodel = YOLO('yolov8s.pt')\n```\n\nThis line loads a pretrained YOLO model from a `.pt` file (`yolov8s.pt`). This method is recommended when you want to continue training a model or use it for inference without starting from scratch.\n\n3. **Building from YAML and Transferring Weights:**\n\n```python\n#model = YOLO('yolov8n.yaml').load('yolov8n.pt')\n```\n\nThis line (currently commented out) first builds a YOLO model using a YAML configuration file (`yolov8n.yaml`) and then transfers the weights from a pretrained model (`yolov8n.pt`). This approach is useful when you want to use a specific architecture but initialize it with weights from a pretrained model.\n\n---\n\n### **Note**\n\nIn the provided code, only the second method (loading a pretrained model) is active, while the other two methods are commented out. Depending on the use case, you can choose the appropriate method to load the YOLO model.","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:40:57.579176Z","iopub.execute_input":"2023-09-27T16:40:57.579573Z","iopub.status.idle":"2023-09-27T16:41:10.48961Z","shell.execute_reply.started":"2023-09-27T16:40:57.57954Z","shell.execute_reply":"2023-09-27T16:41:10.488338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\n#model = YOLO('yolov8n.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8s.pt')  # load a pretrained model (recommended for training)\n#model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:41:10.492824Z","iopub.execute_input":"2023-09-27T16:41:10.493233Z","iopub.status.idle":"2023-09-27T16:41:11.265399Z","shell.execute_reply.started":"2023-09-27T16:41:10.493188Z","shell.execute_reply":"2023-09-27T16:41:11.264201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YAML Configuration File Creation Explanation\n\nThe provided code snippet demonstrates how to create and save a configuration file in YAML format using the `yaml` library in Python.\n\n---\n\n### **1. Importing the YAML Library**\n\n```python\nimport yaml\n```\n\nThis line imports the `yaml` library, which provides functionalities to work with YAML (Yet Another Markup Language) files in Python.\n\n---\n\n### **2. Defining Configuration Data**\n\n```python\nconfig = {\n    'path': '/kaggle/working/car-plate-detection',\n    'train': 'train/images',\n    'val': 'val/images',\n    'nc': 1,\n    'names': ['license'],\n    # 'fl_gamma': 2.0\n}\n```\n\nHere, a Python dictionary named `config` is defined to hold the configuration data. This data includes paths, number of classes, class names, and other potential configurations (like `fl_gamma`, which is currently commented out).\n\n---\n\n### **3. Specifying the File Path**\n\n```python\nyaml_file_path = '/kaggle/working/config.yaml'\n```\n\nThis line defines the file path where the YAML configuration file will be saved.\n\n---\n\n### **4. Writing Data to the YAML File**\n\n```python\nwith open(yaml_file_path, 'w') as yaml_file:\n    yaml.dump(config, yaml_file)\n```\n\nUsing the `with` statement, the code opens the specified file in write mode. The `yaml.dump()` function is then used to write the `config` dictionary data to the file in YAML format.\n\n---\n\n### **5. Printing the Save Location**\n\n```python\nprint(f\"YAML configuration file saved at: {yaml_file_path}\")\n```\n\nThis line simply prints the location where the YAML configuration file has been saved.\n\n---\n\n### **Note**\n\nYAML is a human-readable data serialization format. It's commonly used for configuration files and data exchange between languages with different data structures. The provided code snippet is a simple example of how to create and save a YAML configuration file in Python.","metadata":{}},{"cell_type":"code","source":"import yaml\n\n# Define your configuration data as a Python dictionary\nconfig = {\n    'path': '/kaggle/working/car-plate-detection',\n    'train': 'train/images',  # train images (relative to 'path') 4 images\n    'val': 'val/images',      # val images (relative to 'path') 4 images\n    'nc': 1,                   # Number of classes\n    'names': ['license'],         # Class names\n    # 'fl_gamma': 2.0          # Uncomment this line if needed\n}\n\n# Define the file path where you want to save the YAML file\nyaml_file_path = '/kaggle/working/config.yaml'  # Update with your desired path and file name\n\n# Write the YAML data to the file\nwith open(yaml_file_path, 'w') as yaml_file:\n    yaml.dump(config, yaml_file)\n\nprint(f\"YAML configuration file saved at: {yaml_file_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:41:11.267078Z","iopub.execute_input":"2023-09-27T16:41:11.267559Z","iopub.status.idle":"2023-09-27T16:41:11.279166Z","shell.execute_reply.started":"2023-09-27T16:41:11.267521Z","shell.execute_reply":"2023-09-27T16:41:11.278074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training Explanation\n\nThe provided code snippet demonstrates how to train a YOLO model using the `train` method of the `YOLO` class from the `ultralytics` library.\n\n---\n\n### **Training the YOLO Model**\n\n```python\nresults = model.train(data='/kaggle/working/config.yaml',\n                      epochs=220,\n                      imgsz=640,\n                      pretrained=True,\n                      name=\"license_plate_small3\",\n                      patience=35,\n                      flipud=0.5,\n                      batch=16,\n                      optimizer='SGD',\n                      lr0=0.001,\n                      augment=True)\n```\n\nHere's a breakdown of the parameters used in the `train` method:\n\n- **data**: Path to the YAML configuration file that contains dataset information.\n\n- **epochs**: Number of training epochs. An epoch is one complete forward and backward pass of all the training examples.\n\n- **imgsz**: Image size. The model will resize training images to this size.\n\n- **pretrained**: If set to `True`, the model will use pretrained weights. This can help in achieving better accuracy faster.\n\n- **name**: Name of the training run. Useful for distinguishing between different training sessions.\n\n- **patience**: Number of epochs to wait for improvement before stopping training. Helps in early stopping if the model isn't improving.\n\n- **flipud**: Probability of flipping an image upside down during training. Data augmentation technique to improve model robustness.\n\n- **batch**: Batch size for training. Number of training examples utilized in one iteration.\n\n- **optimizer**: The optimization algorithm to use. In this case, Stochastic Gradient Descent (SGD) is used.\n\n- **lr0**: Initial learning rate. Determines the step size at each iteration while moving towards a minimum of the loss function.\n\n- **augment**: If set to `True`, data augmentation techniques will be applied during training. This can help in improving model accuracy.\n\n---\n\n### **Note**\n\nThe `train` method returns a `results` object which contains information about the training process, such as loss values, accuracy metrics, etc. This can be used later for analysis and visualization.","metadata":{}},{"cell_type":"code","source":"results = model.train(data='/kaggle/working/config.yaml',\n                      epochs=220,\n                      imgsz=640,\n                      pretrained = True ,\n                      name= \"license_plate_small3\",\n                      patience = 35,\n                      flipud=0.5,\n                      batch = 16,\n                      optimizer = 'SGD',\n                      lr0 = 0.001,\n                      augment = True\n                      )","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:41:11.280563Z","iopub.execute_input":"2023-09-27T16:41:11.280968Z","iopub.status.idle":"2023-09-27T16:54:18.531275Z","shell.execute_reply.started":"2023-09-27T16:41:11.280934Z","shell.execute_reply":"2023-09-27T16:54:18.529617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat /kaggle/working/car-plate-detection/val/labels/Cars4.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:18.54641Z","iopub.execute_input":"2023-09-27T16:54:18.550751Z","iopub.status.idle":"2023-09-27T16:54:20.225805Z","shell.execute_reply.started":"2023-09-27T16:54:18.550698Z","shell.execute_reply":"2023-09-27T16:54:20.224566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Validation Explanation\n\nThe provided code snippet demonstrates how to validate a YOLO model using the `val` method of the `YOLO` class from the `ultralytics` library.\n\n---\n\n### **1. Loading the YOLO Model**\n\n```python\nmodel = YOLO('/kaggle/working/car-plate-detection/runs/detect/license_plate_small3/weights/best.pt')\n```\n\nThis line loads a pretrained YOLO model from a `.pt` file. The path provided points to the best weights obtained during a previous training session.\n\n---\n\n### **2. Default Model Validation**\n\n```python\nresults = model.val()\n```\n\nThis line validates the model using default parameters. The `val` method returns a `results` object containing information about the validation process, such as precision, recall, and F1 score.\n\n---\n\n### **3. Model Validation with Different Confidence Thresholds**\n\n```python\nfor i in [0.25, 0.15, 0.05]:\n    results = model.val(name=f'confidence: {i}', conf=i, iou=0.8)\n```\n\nThis loop validates the model three times, each with a different confidence threshold (`conf`). The confidence threshold determines which detections are considered by the model. Detections with a confidence score below this threshold are discarded.\n\n- **name**: A custom name for the validation run. Useful for distinguishing between different validation sessions.\n\n- **conf**: The confidence threshold for detections.\n\n- **iou**: Intersection over Union threshold. Determines how much overlap there should be between the predicted bounding box and the ground truth bounding box for a detection to be considered correct.\n\nIn this case, the model is validated with confidence thresholds of 0.25, 0.15, and 0.05, while keeping the IoU threshold constant at 0.8.\n\n---\n\n### **Note**\n\nValidating the model with different confidence thresholds can help in understanding how the model performs under different conditions. It can provide insights into the optimal confidence threshold to use for the specific application.","metadata":{}},{"cell_type":"code","source":"model = YOLO('/kaggle/working/car-plate-detection/runs/detect/license_plate_small3/weights/best.pt')\nresults = model.val()\nfor i in [0.25,0.15,0.05]:\n  results = model.val(name= f'cofidence: {i}', conf= i , iou=0.8)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:20.22823Z","iopub.execute_input":"2023-09-27T16:54:20.229005Z","iopub.status.idle":"2023-09-27T16:54:48.42726Z","shell.execute_reply.started":"2023-09-27T16:54:20.228955Z","shell.execute_reply":"2023-09-27T16:54:48.425643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = YOLO('/kaggle/working/car-plate-detection/runs/detect/license_plate_small3/weights/best.pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:48.429785Z","iopub.execute_input":"2023-09-27T16:54:48.434912Z","iopub.status.idle":"2023-09-27T16:54:48.562556Z","shell.execute_reply.started":"2023-09-27T16:54:48.43486Z","shell.execute_reply":"2023-09-27T16:54:48.561414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Python Libraries Import Explanation\n\nThe provided code snippet imports various Python libraries and modules that are commonly used in data analysis, image processing, and visualization tasks.\n\n---\n\n### **1. Data Analysis Libraries**\n\n```python\nimport pandas as pd\nimport numpy as np\n```\n\n- `pandas`: A powerful library for data manipulation and analysis. It provides data structures like DataFrame for handling and analyzing structured data.\n\n- `numpy`: A library for numerical computing in Python. It provides support for arrays (including multidimensional arrays), as well as a collection of mathematical functions to operate on these arrays.\n\n---\n\n### **2. Image Processing and Visualization Libraries**\n\n```python\nimport PIL\nfrom PIL import Image\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport cv2\n```\n\n- `PIL` and `Image`: Modules from the Python Imaging Library (PIL), which provides capabilities to open, manipulate, and save various image file formats.\n\n- `display`: A function from IPython's display module to display PIL images, DataFrames, and other objects in Jupyter notebooks.\n\n- `matplotlib.pyplot`: A module in the Matplotlib library for plotting and visualization in Python.\n\n- `cv2`: OpenCV library, which provides tools for image and video analysis, including object detection, image segmentation, and more.\n\n---\n\n### **3. File Handling and Miscellaneous**\n\n```python\nfrom glob import glob\nimport random\nimport warnings\nwarnings.simplefilter('ignore')\n```\n\n- `glob`: A function to search for files based on patterns and wildcards.\n\n- `random`: A module that implements pseudo-random number generators for various distributions.\n\n- `warnings`: A module to handle warnings. The `simplefilter('ignore')` line is used to suppress warnings, ensuring they don't clutter the output.\n\n---\n\n### **Note**\n\nIt's a good practice to import all necessary libraries at the beginning of a script or notebook. This ensures that all dependencies are available before executing subsequent code.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport PIL \nfrom PIL import Image\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport random\nimport cv2\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:48.567674Z","iopub.execute_input":"2023-09-27T16:54:48.568529Z","iopub.status.idle":"2023-09-27T16:54:48.583649Z","shell.execute_reply.started":"2023-09-27T16:54:48.568489Z","shell.execute_reply":"2023-09-27T16:54:48.581097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Image Display Explanation\n\nThe provided code snippet demonstrates how to randomly select and display images from a specified directory using Python libraries.\n\n---\n\n### **1. Setting the Image Directory and Number of Samples**\n\n```python\nroot_path = '/kaggle/working/car-plate-detection/val/images/*'\nnum_samples = 4\n```\n\n- `root_path`: Specifies the directory path where the images are stored. The wildcard `*` at the end means it will consider all files in that directory.\n\n- `num_samples`: Specifies the number of random images to display.\n\n---\n\n### **2. Fetching Image Paths and Randomly Selecting Images**\n\n```python\nimages_data = glob(root_path)\nrandom_image = random.sample(images_data, num_samples)\n```\n\n- `glob(root_path)`: Fetches the paths of all files in the specified directory.\n\n- `random.sample()`: Randomly selects a specified number of items from a list. In this case, it selects `num_samples` image paths from `images_data`.\n\n---\n\n### **3. Displaying the Randomly Selected Images**\n\n```python\nplt.figure(figsize=(12,10))\nfor i in range(num_samples):\n    plt.subplot(2,2,i+1)\n    plt.imshow(cv2.imread(random_image[i]))\n    plt.axis('off')\n```\n\n- `plt.figure()`: Initializes a new figure for plotting. The `figsize` parameter sets the width and height of the figure in inches.\n\n- `plt.subplot()`: Divides the figure into a grid (in this case, 2x2) and selects a specific subplot to plot on. The loop ensures that each image is plotted on a separate subplot.\n\n- `cv2.imread()`: Reads an image from the specified path.\n\n- `plt.imshow()`: Displays the image on the current subplot.\n\n- `plt.axis('off')`: Hides the axis labels and ticks for a cleaner display.\n\n---\n\n### **Note**\n\nThis code snippet is useful for quickly visualizing a random subset of images from a dataset, which can be helpful in understanding the nature and quality of the data.","metadata":{}},{"cell_type":"code","source":"root_path = '/kaggle/working/car-plate-detection/val/images/*'\nnum_samples = 4\nimages_data = glob(root_path)\nrandom_image = random.sample(images_data, num_samples)\n\nplt.figure(figsize=(12,10))\nfor i in range(num_samples):\n    plt.subplot(2,2,i+1)\n    plt.imshow(cv2.imread(random_image[i]))\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:48.588604Z","iopub.execute_input":"2023-09-27T16:54:48.589235Z","iopub.status.idle":"2023-09-27T16:54:49.409988Z","shell.execute_reply.started":"2023-09-27T16:54:48.589196Z","shell.execute_reply":"2023-09-27T16:54:49.408869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YOLO Model Prediction and Visualization Explanation\n\nThe provided code snippet demonstrates how to use a YOLO model to predict and visualize object detections on a set of randomly selected images.\n\n---\n\n### **1. Initializing an Empty List for Images**\n\n```python\nimages = []\n```\n\nThis line initializes an empty list named `images` which will be used to store the visualized output images with detected objects.\n\n---\n\n### **2. Looping Through the Randomly Selected Images**\n\n```python\nfor i in range(num_samples):\n```\n\nThis loop iterates through each of the randomly selected images.\n\n---\n\n### **3. Predicting Objects in the Image**\n\n```python\nyolo_outputs = model.predict(random_image[i])\noutput = yolo_outputs[0]\n```\n\n- `model.predict()`: Uses the YOLO model to predict objects in the given image.\n\n- `output`: Extracts the first output from the prediction, which contains information about the detected objects.\n\n---\n\n### **4. Extracting Detection Information**\n\n```python\nbox = output.boxes\nnames = output.names\n```\n\n- `box`: Contains bounding box information of the detected objects.\n\n- `names`: Contains the names (or labels) of the detected objects.\n\n---\n\n### **5. Displaying Detection Details**\n\nThe inner loop iterates through each detected object and prints details such as the label, coordinates of the bounding box, and the confidence score.\n\n---\n\n### **6. Storing the Visualized Image**\n\n```python\nimages.append(output.plot()[:, :, ::-1])\n```\n\n- `output.plot()`: Visualizes the detected objects on the image.\n\n- `[:, :, ::-1]`: Converts the image from BGR to RGB format (as OpenCV reads images in BGR format by default).\n\n- `images.append()`: Appends the visualized image to the `images` list.\n\n---\n\n### **Note**\n\nThis code snippet is useful for visualizing the performance of the YOLO model on a random subset of images. It provides insights into the detected objects, their locations, and the model's confidence in each detection.","metadata":{}},{"cell_type":"code","source":"images = []\nfor i in range(num_samples):\n    yolo_outputs = model.predict(random_image[i])\n    output = yolo_outputs[0]\n    box = output.boxes\n    names = output.names\n    print('**********************')\n    for j in range(len(box)):\n        labels = names[box.cls[j].item()]\n        coordinates = box.xyxy[j].tolist()\n        confidence = np.round(box.conf[j].item(), 2)\n        print(f'In this image {len(box)} License plate has been detected.')\n        print(f'License {j + 1} is: {labels}')\n        print(f'Coordinates are: {coordinates}')\n        print(f'Confidence is: {confidence}')\n        print('-------')\n        \n    # Store the image in the 'images' list\n    images.append(output.plot()[:, :, ::-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:49.411017Z","iopub.execute_input":"2023-09-27T16:54:49.411312Z","iopub.status.idle":"2023-09-27T16:54:50.038074Z","shell.execute_reply.started":"2023-09-27T16:54:49.411285Z","shell.execute_reply":"2023-09-27T16:54:50.036996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nfor i, img in enumerate(images):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(img)\n    plt.axis('off') ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:50.039651Z","iopub.execute_input":"2023-09-27T16:54:50.043406Z","iopub.status.idle":"2023-09-27T16:54:50.905889Z","shell.execute_reply.started":"2023-09-27T16:54:50.043346Z","shell.execute_reply":"2023-09-27T16:54:50.904454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.read_csv('/kaggle/working/car-plate-detection/runs/detect/license_plate_small3/results.csv')\nresult.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:50.91492Z","iopub.execute_input":"2023-09-27T16:54:50.91532Z","iopub.status.idle":"2023-09-27T16:54:50.993684Z","shell.execute_reply.started":"2023-09-27T16:54:50.915283Z","shell.execute_reply":"2023-09-27T16:54:50.992707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove leading and trailing spaces from column names\nresult.columns = result.columns.str.strip()\n\nepoch_column = result['epoch']\nbox_train_losses = result['train/box_loss']\nbox_val_losses = result['val/box_loss']\ncls_train_losses = result['train/cls_loss']\ncls_val_losses = result['val/cls_loss']\n\nplt.figure(figsize=(12,5))\nplt.style.use('ggplot')  # You can choose a style you prefer\nplt.subplot(1,2,1)\nplt.plot(epoch_column, box_train_losses, label='train_losses')\nplt.plot(epoch_column, box_val_losses, label='val_losses')\nplt.grid(True, linestyle='--', linewidth=0.5, color='gray')# Add a grid\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train and Validation Box Losses')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epoch_column, cls_train_losses, label='train_losses')\nplt.plot(epoch_column, cls_val_losses, label='val_losses')\nplt.grid(True, linestyle='--', linewidth=0.5, color='gray')# Add a grid\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train and Validation Class Losses')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:50.995251Z","iopub.execute_input":"2023-09-27T16:54:50.995817Z","iopub.status.idle":"2023-09-27T16:54:51.7866Z","shell.execute_reply.started":"2023-09-27T16:54:50.995786Z","shell.execute_reply":"2023-09-27T16:54:51.785476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.imshow(cv2.imread('/kaggle/working/car-plate-detection/runs/detect/license_plate_small3/results.png'))\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:51.79109Z","iopub.execute_input":"2023-09-27T16:54:51.794536Z","iopub.status.idle":"2023-09-27T16:54:52.70231Z","shell.execute_reply.started":"2023-09-27T16:54:51.794497Z","shell.execute_reply":"2023-09-27T16:54:52.701237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Text Extraction and Visualization Explanation\n\nThe provided code snippet demonstrates how to use a combination of YOLO for object detection and Tesseract for Optical Character Recognition (OCR) to extract text from detected objects in images.\n\n---\n\n### **1. Importing Necessary Libraries**\n\nThe code starts by importing necessary libraries such as `cv2`, `pytesseract`, `PIL`, `numpy`, `matplotlib`, `glob`, and `random`.\n\n---\n\n### **2. Defining the Text Extraction Function**\n\n```python\ndef extract_text_from_image(image, coordinates):\n    ...\n```\n\nThis function extracts text from a specified region of an image using the Tesseract OCR engine. The region is defined by the provided bounding box `coordinates`.\n\n---\n\n### **3. Setting the Image Directory and Number of Samples**\n\n```python\nroot_path = '/kaggle/working/car-plate-detection/val/images/*'\nnum_samples = 2\n```\n\nThis sets the directory path for the images and the number of random images to process.\n\n---\n\n### **4. Randomly Selecting Images**\n\n```python\nimages_data = glob(root_path)\nrandom_image_paths = random.sample(images_data, num_samples)\n```\n\nThis code fetches all image paths from the specified directory and then randomly selects a subset of them.\n\n---\n\n### **5. Processing and Displaying the Images**\n\nThe main loop processes each randomly selected image. For each image:\n\n- The YOLO model predicts objects.\n\n- Detected license plates' details (label, coordinates, confidence) are printed.\n\n- The `extract_text_from_image` function is called to extract text from the detected license plate using OCR.\n\n- The processed image is displayed using `matplotlib`.\n\n---\n\n### **Note**\n\nThis code snippet provides an end-to-end workflow for detecting license plates in images and then extracting text from them using OCR. It's a common approach for applications like automatic license plate recognition.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport pytesseract\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport random\n\ndef extract_text_from_image(image, coordinates):\n    \"\"\"\n    Extract text from the given image using the provided coordinates.\n    \n    Args:\n    - image (numpy array): The image from which to extract text.\n    - coordinates (list): The bounding box coordinates [xmin, ymin, xmax, ymax].\n    \n    Returns:\n    - str: The extracted text.\n    \"\"\"\n    xmin, ymin, xmax, ymax = map(int, coordinates)\n    roi = image[ymin:ymax, xmin:xmax]\n    text = pytesseract.image_to_string(roi, config='--psm 8')\n    return text.strip()\n\nroot_path = '/kaggle/working/car-plate-detection/val/images/*'\nnum_samples = 2\nimages_data = glob(root_path)\nrandom_image_paths = random.sample(images_data, num_samples)\n\nplt.figure(figsize=(12,10))\nfor i in range(num_samples):\n    image_np = cv2.imread(random_image_paths[i])\n    yolo_outputs = model.predict(image_np)\n    output = yolo_outputs[0]\n    box = output.boxes\n    names = output.names\n    print('**********************')\n    for j in range(len(box)):\n        labels = names[box.cls[j].item()]\n        coordinates = box.xyxy[j].tolist()\n        confidence = np.round(box.conf[j].item(), 2)\n        print(f'In this image {len(box)} License plate has been detected.')\n        print(f'License {j + 1} is: {labels}')\n        print(f'Coordinates are: {coordinates}')\n        print(f'Confidence is: {confidence}')\n        \n        # Extract text using OCR\n        extracted_text = extract_text_from_image(image_np, coordinates)\n        print(f'Extracted text from license plate: {extracted_text}')\n        \n        print('-------')\n        \n    # Display the image\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for displaying with matplotlib\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T16:54:52.707099Z","iopub.execute_input":"2023-09-27T16:54:52.707884Z","iopub.status.idle":"2023-09-27T16:54:53.833673Z","shell.execute_reply.started":"2023-09-27T16:54:52.707844Z","shell.execute_reply":"2023-09-27T16:54:53.832656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
